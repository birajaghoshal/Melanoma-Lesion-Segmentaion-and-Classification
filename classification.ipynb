{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import merge\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from skimage.transform import rescale, resize\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disp(I):\n",
    "    I = np.uint8(I)\n",
    "    cv2.imshow('image', I)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "melonoma_input_folder = '../Dataset/Images/melonoma-resized'\n",
    "others_input_folder = '../Dataset/Images/others-resized'\n",
    "ground_truth_input_folder = '../Dataset/Images/gt-resized'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data \n",
    "* Read the images from the **(384, 384)** resized directory to preserve the square shape.\n",
    "* Multiply them with their respective ground truths.\n",
    "* Resize it to **(64, 64)**.\n",
    "* Save it in seperate directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "num_melonoma = len(os.listdir(melonoma_input_folder))\n",
    "for file_name in os.listdir(melonoma_input_folder):\n",
    "    train_img = cv2.imread(os.path.join(melonoma_input_folder, file_name))\n",
    "    ground_truth = cv2.imread(os.path.join(ground_truth_input_folder, file_name[:12] + '_segmentation.png'))\n",
    "    ground_truth[ground_truth >= 127] = 255\n",
    "    ground_truth[ground_truth < 127] = 0\n",
    "    ground_truth = ground_truth / 255\n",
    "    if iterator < 0.9 * num_melonoma:\n",
    "        cv2.imwrite(os.path.join('../Dataset/Classification_Data/Training/class_1_melonoma', file_name), cv2.resize(train_img * ground_truth, (64, 64)))\n",
    "    else:\n",
    "        cv2.imwrite(os.path.join('../Dataset/Classification_Data/Testing/class_1_melonoma', file_name), cv2.resize(train_img * ground_truth, (64, 64)))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = 0\n",
    "num_others = len(os.listdir(others_input_folder))\n",
    "for file_name in os.listdir(others_input_folder):\n",
    "    train_img = cv2.imread(os.path.join(others_input_folder, file_name))\n",
    "    ground_truth = cv2.imread(os.path.join(ground_truth_input_folder, file_name[:12] + '_segmentation.png'))\n",
    "    ground_truth[ground_truth >= 127] = 255\n",
    "    ground_truth[ground_truth < 127] = 0\n",
    "    ground_truth = ground_truth / 255\n",
    "    if iterator < 0.9 * num_others:\n",
    "        cv2.imwrite(os.path.join('../Dataset/Classification_Data/Training/class_0_others', file_name), cv2.resize(train_img * ground_truth, (64, 64)))\n",
    "    else:\n",
    "        cv2.imwrite(os.path.join('../Dataset/Classification_Data/Testing/class_0_others', file_name), cv2.resize(train_img * ground_truth, (64, 64)))\n",
    "    iterator = iterator + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1.0 / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CNN_input_size = [64, 64, 3]\n",
    "trained_classifier_path = '../Trained/classifier.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f1-loss\n",
    "* Tried this loss\n",
    "* Inferior results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    TP = K.sum(y_true * y_pred)\n",
    "    soft_precision_positive = TP / (K.sum(y_pred) + 1e-7)\n",
    "    soft_recall_positive = TP / (K.sum(y_true) + 1e-7)\n",
    "    f1_loss_positive = 2 * soft_precision_positive * soft_recall_positive / (soft_precision_positive + soft_recall_positive + 1e-7)\n",
    "    TN = K.sum((1 - y_true) * (1 - y_pred))\n",
    "    soft_precision_negative = TN / (K.sum(1 - y_pred) + 1e-7)\n",
    "    soft_recall_negative = TN / (K.sum(1 - y_true) + 1e-7)\n",
    "    f1_loss_negative = 2 * soft_precision_negative * soft_recall_negative / (soft_precision_negative + soft_recall_negative + 1e-7)\n",
    "    return 1 - (0.99 * f1_loss_positive + 0.01 * f1_loss_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Binary Cross-Entropy\n",
    "* Tried this loss\n",
    "* Inferior results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    return -K.sum(0.99 * y_true * K.log(y_pred + 1e-7) + 0.01 * (1 - y_true) * K.log(1 - y_pred + 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture\n",
    "* Tried VGG-19 architecture\n",
    "* Always predicts 0 resulting in 0 f1-score\n",
    "* The below simple architecture gives decent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 94,433\n",
      "Trainable params: 94,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv1'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv2'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv3'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv4'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation = 'relu', name = 'dense_1'))\n",
    "model.add(Dense(1, name = 'dense_2'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(trained_classifier_path, monitor = 'val_loss', save_best_only = True, verbose = 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1801 images belonging to 2 classes.\n",
      "Found 199 images belonging to 2 classes.\n",
      "Epoch 1/25\n",
      "450/450 [==============================] - 61s 136ms/step - loss: 0.5819 - acc: 0.8122 - val_loss: 0.5441 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54412, saving model to ../Trained/classifier.h5\n",
      "Epoch 2/25\n",
      "450/450 [==============================] - 63s 139ms/step - loss: 0.5628 - acc: 0.8134 - val_loss: 0.5804 - val_acc: 0.8141\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/25\n",
      "450/450 [==============================] - 63s 139ms/step - loss: 0.5546 - acc: 0.8159 - val_loss: 0.5282 - val_acc: 0.7889\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.54412 to 0.52824, saving model to ../Trained/classifier.h5\n",
      "Epoch 4/25\n",
      "450/450 [==============================] - 71s 157ms/step - loss: 0.5399 - acc: 0.8185 - val_loss: 0.5156 - val_acc: 0.7990\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.52824 to 0.51557, saving model to ../Trained/classifier.h5\n",
      "Epoch 5/25\n",
      "450/450 [==============================] - 61s 137ms/step - loss: 0.5103 - acc: 0.8383 - val_loss: 0.5882 - val_acc: 0.7437\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/25\n",
      "450/450 [==============================] - 62s 138ms/step - loss: 0.4766 - acc: 0.8532 - val_loss: 0.5597 - val_acc: 0.7236\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/25\n",
      "450/450 [==============================] - 62s 137ms/step - loss: 0.4417 - acc: 0.8725 - val_loss: 0.6504 - val_acc: 0.6985\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/25\n",
      "450/450 [==============================] - 63s 141ms/step - loss: 0.4024 - acc: 0.8850 - val_loss: 0.7120 - val_acc: 0.7136\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/25\n",
      "450/450 [==============================] - 60s 134ms/step - loss: 0.3950 - acc: 0.8851 - val_loss: 0.7355 - val_acc: 0.7035\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/25\n",
      "450/450 [==============================] - 59s 132ms/step - loss: 0.3717 - acc: 0.8959 - val_loss: 0.7635 - val_acc: 0.6935\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/25\n",
      "450/450 [==============================] - 60s 132ms/step - loss: 0.3670 - acc: 0.9015 - val_loss: 0.8359 - val_acc: 0.6482\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/25\n",
      "450/450 [==============================] - 61s 136ms/step - loss: 0.3622 - acc: 0.9043 - val_loss: 0.9057 - val_acc: 0.7085\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/25\n",
      "450/450 [==============================] - 63s 140ms/step - loss: 0.3643 - acc: 0.9034 - val_loss: 1.0856 - val_acc: 0.7286\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/25\n",
      "450/450 [==============================] - 61s 137ms/step - loss: 0.3677 - acc: 0.9005 - val_loss: 0.8321 - val_acc: 0.6633\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/25\n",
      "450/450 [==============================] - 62s 137ms/step - loss: 0.3678 - acc: 0.9004 - val_loss: 0.9179 - val_acc: 0.6784\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/25\n",
      "450/450 [==============================] - 61s 136ms/step - loss: 0.3544 - acc: 0.9054 - val_loss: 1.1187 - val_acc: 0.6482\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/25\n",
      "450/450 [==============================] - 61s 136ms/step - loss: 0.3743 - acc: 0.8989 - val_loss: 0.8410 - val_acc: 0.6884\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/25\n",
      "424/450 [===========================>..] - ETA: 3s - loss: 0.3548 - acc: 0.9047"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-a5378cc24da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                     class_mode = 'binary', target_size = (64, 64)), steps_per_epoch = 1800 / 4, epochs = 25, \n\u001b[1;32m      3\u001b[0m                     verbose = 1, validation_data = datagen.flow_from_directory('../Dataset/Classification_Data/Testing/', \n\u001b[0;32m----> 4\u001b[0;31m                     batch_size = 4, class_mode = 'binary',  target_size = (64, 64)), callbacks = callbacks)\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1274\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2224\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\vignesh2496\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow_from_directory('../Dataset/Classification_Data/Training/', batch_size = 16, \n",
    "                    class_mode = 'binary', target_size = (64, 64)), steps_per_epoch = 1800 / 4, epochs = 25, \n",
    "                    verbose = 1, validation_data = datagen.flow_from_directory('../Dataset/Classification_Data/Testing/', \n",
    "                    batch_size = 4, class_mode = 'binary',  target_size = (64, 64)), callbacks = callbacks, class_weight = {0:1, 1:8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 199 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = datagen.flow_from_directory('../Dataset/Classification_Data/Testing/', target_size = (64, 64), class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for melanoma: 0.338028169014\n",
      "F1 score for non-melanoma: 0.85626911315\n"
     ]
    }
   ],
   "source": [
    "results = model.predict_generator(test_set)\n",
    "y_true = test_set.classes\n",
    "y_pred = np.rint(results)\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "# Class - Melanoma\n",
    "precision = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "recall =  cm[1][1] / (cm[1][1] + cm[1][0])\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(\"F1 score for melanoma:\", f1)\n",
    "\n",
    "# Class - Others\n",
    "precision = cm[0][0] / (cm[0][0] + cm[0][1])\n",
    "recall = cm[0][0] / (cm[0][0] + cm[1][0])\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(\"F1 score for non-melanoma:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f1-score for Melonoma : 0.338  \n",
    "## f1-score for Non-melonoma : 0.856"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
